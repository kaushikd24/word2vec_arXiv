{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Continous Bag of Worlds Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/kaushikdwivedi/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1161192"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = words[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(some_words):\n",
    "    tokenized_words = []\n",
    "    \n",
    "    for word in some_words:\n",
    "        word = word.lower()\n",
    "        word = re.sub(r\"'\",\"\", word)\n",
    "        \n",
    "        clean_parts = re.findall(r\"\\w+\", word)\n",
    "        tokenized_words.extend(clean_parts)\n",
    "        \n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenize(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89563"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"'\" in words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i<len(words):\n",
    "    vocab[i] = words[i]\n",
    "    i = i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "emb_dim = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_to_word = vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index = {word: idx for idx, word in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the tokenized corpus\n",
    "tokenized_corpus = []\n",
    "for i in index_to_word:\n",
    "    tokenized_corpus.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.linear = nn.Linear(emb_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, context_batch):\n",
    "        context_embeds = self.embeddings(context_batch)\n",
    "        v_ctx = context_embeds.mean(dim=1)\n",
    "        logits  = self.linear(v_ctx)\n",
    "        return logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cbow_batches(tokenized_corpus, window_size=2, batch_size=128):\n",
    "    context_batch = []\n",
    "    target_batch = []\n",
    "    \n",
    "    for i in range(window_size, len(tokenized_corpus) - window_size):\n",
    "        context = tokenized_corpus[i - window_size:i] + tokenized_corpus[i + 1:i + window_size + 1]\n",
    "        target = tokenized_corpus[i]\n",
    "\n",
    "        context_batch.append(context)\n",
    "        target_batch.append(target)\n",
    "\n",
    "        if len(context_batch) == batch_size:\n",
    "            yield torch.tensor(context_batch, dtype=torch.long), torch.tensor(target_batch, dtype=torch.long)\n",
    "            context_batch, target_batch = [], []\n",
    "\n",
    "    if context_batch:\n",
    "        yield torch.tensor(context_batch, dtype=torch.long), torch.tensor(target_batch, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 50\n",
    "num_epochs =1\n",
    "batch_size = 128\n",
    "lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOW(vocab_size, emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Epoch 1/1\n",
      "Step 0 || Batch Loss: 9.3239\n",
      "Step 100 || Batch Loss: 10.4122\n",
      "Step 200 || Batch Loss: 11.5879\n",
      "Step 300 || Batch Loss: 12.0860\n",
      "Step 400 || Batch Loss: 12.5950\n",
      "Step 500 || Batch Loss: 13.1339\n",
      "Step 600 || Batch Loss: 13.6471\n",
      "✅ Epoch 1 Finished || Avg Loss: 12.1607\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    step_count = 0  # 🧮 Track number of steps\n",
    "\n",
    "    print(f\"\\n🔁 Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for step, (context_batch, target_batch) in enumerate(generate_cbow_batches(tokenized_corpus, window_size=2, batch_size=128)):\n",
    "        logits = model(context_batch)\n",
    "        loss = F.cross_entropy(logits, target_batch)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        step_count += 1\n",
    "\n",
    "        # 🖨️ Print every 100 steps\n",
    "        if step % 100 == 0:\n",
    "            print(f\"Step {step} || Batch Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / step_count\n",
    "    print(f\"✅ Epoch {epoch+1} Finished || Avg Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neighbors(word, word_to_ix, ix_to_word, model, top_k=5):\n",
    "    with torch.no_grad():\n",
    "        word_idx = word_to_ix[word]\n",
    "        word_vec = model.embeddings(torch.tensor([word_idx])).squeeze()\n",
    "        all_vecs = model.embeddings.weight\n",
    "        sims = torch.matmul(all_vecs, word_vec)\n",
    "        topk = torch.topk(sims, top_k + 1)  # +1 to skip the word itself\n",
    "\n",
    "        for idx in topk.indices[1:]:\n",
    "            print(ix_to_word[idx.item()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and\n",
      "and\n",
      "was\n",
      "to\n",
      "said\n"
     ]
    }
   ],
   "source": [
    "get_neighbors(\"india\", word_to_index, index_to_word, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
